{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional PixelCNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIHrnUMxnzxM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modifications needed for PixelCNN\n",
    "Some Points :\n",
    "1. ResBlock Architecture as shown in Figure 5 of PixelRNN Paper : https://arxiv.org/pdf/1601.06759.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODcFuMwOn7D5"
   },
   "outputs": [],
   "source": [
    "class MaskConv2D(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, color_conditioning = False, conditional_size = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', torch.zeros(self.weight.size())) # Weight shape is same as tensor shape\n",
    "        self.conditional_size = conditional_size\n",
    "        assert mask_type in ['A', 'B'], \"Unknown Mask Type\"\n",
    "        h =  self.kernel_size[0]\n",
    "        w = self.kernel_size[1]\n",
    "        \n",
    "        # Creating masks for autoregressive properties\n",
    "        self.mask[:, :, :h//2, :] = 1  # Mask type A \n",
    "        self.mask[:, :, h//2, :w//2 + (mask_type == 'B')] = 1 # Mask type B\n",
    "        \n",
    "        # Adding autoregressive property of color channels\n",
    "        if color_conditioning:\n",
    "            in_third, out_third = self.in_channels // 3, self.out_channels // 3\n",
    "            if mask_type == 'B':\n",
    "                self.mask[2*out_third:, :, h // 2, w // 2] = 1 # B has connections from R, G and B of input mask\n",
    "                self.mask[out_third:2*out_third, :2*in_third, h // 2, w // 2] = 1 # G has connections from R and G\n",
    "                self.mask[out_third:, in_third:, h // 2, w // 2] = 1 # R has connections only from R\n",
    "            else:\n",
    "                self.mask[out_third:2*out_third, :in_third, h // 2, w // 2] = 1  # G has connections from R\n",
    "                self.mask[2*out_third:, :2*in_third, h // 2, w // 2] = 1 # B has connections from R and G\n",
    "                \n",
    "        if self.conditional_size:\n",
    "            if len(self.conditional_size) == 1:\n",
    "                self.cond_op = nn.Linear(conditional_size[0], self.out_channels)\n",
    "            else:\n",
    "                self.cond_op = nn.Conv2d(conditional_size[0], self.out_channels, stride = 1,\n",
    "                                         kernel_size = 3, padding = 1)\n",
    "      \n",
    "    def forward(self, x, cond = None):\n",
    "        self.weight.data *= self.mask\n",
    "        out = super(MaskConv2D, self).forward(x)\n",
    "        if self.conditional_size:\n",
    "            if len(self.conditional_size) == 1:\n",
    "                out = out + self.cond_op(cond).view(x.shape[0], -1, 1, 1)\n",
    "            else:\n",
    "                out = out + self.cond_op(cond)\n",
    "        return out\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, **kwargs):\n",
    "        self.net = nn.ModuleList([\n",
    "            nn.Relu(),\n",
    "            MaskConv2D('B', in_channels, in_channels // 2, 1, 1, 1 // 2, **kwargs),\n",
    "            nn.ReLU(),\n",
    "            MaskConv2D('B', in_channels // 2, in_channels // 2, 1, 7, 7 // 2, **kwargs),\n",
    "            nn.ReLU(),\n",
    "            MaskConv2D('B', in_channels // 2, in_channels, 1, 1, 1 // 2, **kwargs),\n",
    "        ])\n",
    "    def forward(self, x, cond = None):\n",
    "        out = self.net(x)\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, MaskConv2D):\n",
    "                out = layer(out, cond = cond)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        return x + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q67cuNEknKlD"
   },
   "outputs": [],
   "source": [
    "class Conditional_PixelCNN(nn.Module):\n",
    "    def __init__(self, input_shape, channels, colors, no_of_layers,\n",
    "                 color_conditioning, use_ResBlock, conditional_size = None, device = None):\n",
    "        super(Conditional_PixelCNN, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.device = device\n",
    "        self.channels = channels\n",
    "        self.color_channels = colors\n",
    "        self.color_conditioning = color_conditioning\n",
    "        self.conditional_size = conditional_size\n",
    "        \n",
    "        # Define kwargs based on input\n",
    "        kwargs = dict(\n",
    "            color_conditioning = self.color_conditioning,\n",
    "            conditional_size = self.conditional_size\n",
    "        )\n",
    "        \n",
    "        # Initialize block function to be used repeatedly\n",
    "        if use_ResBlock:\n",
    "            block = lambda: ResBlock(channels, **kwargs)    \n",
    "        else:\n",
    "            block = lambda: MaskConv2D('B', True, conditional_size, channels, channels, 1, 7, 7 // 2, **kwargs)\n",
    "        \n",
    "        # 7 x 7 Conv2D operation using Type A Mask\n",
    "        kernel_size = 7\n",
    "        self.layers = []\n",
    "        self.layers.extend([MaskConv2D('A', input_shape[0], channels, 1, kernel_size, kernel_size // 2, **kwargs)])\n",
    "        \n",
    "        # 5 7 x 7 Conv2D operation using Type B Mask\n",
    "        for _ in range(no_of_layers):\n",
    "            self.layers.extend([F.relu(),\n",
    "                                nn.BatchNorm2d(channels),\n",
    "                                block(),])\n",
    "        \n",
    "        # 2 1 x 1 Conv2D operation using Type B Mask\n",
    "        kernel_size = 1\n",
    "        self.layers.extend([F.relu(),\n",
    "                            nn.BatchNorm2d(channels),\n",
    "                            MaskConv2D('B', True, channels, channels, stride = 1,\n",
    "                                       kernel_size = kernel_size, **kwargs),\n",
    "                            F.relu(),\n",
    "                            nn.BatchNorm2d(channels),\n",
    "                            MaskConv2D('B', True, channels, self.color_channels * self.input_shape[0],\n",
    "                                       stride = 1, kernel_size = kernel_size, **kwargs)])\n",
    "        self.net = nn.ModuleList(*self.layers)\n",
    "        \n",
    "        if self.conditional_size:\n",
    "            if len(self.conditional_size) == 1:\n",
    "                self.cond_op = lambda x: x  # identity\n",
    "            else:\n",
    "                self.cond_op = nn.Sequential(\n",
    "            F.relu(nn.Conv2d(1, 64, 3, padding=1)),\n",
    "            F.relu(nn.Conv2d(64, 64, 3, padding=1)),\n",
    "            F.relu(nn.Conv2d(64, 64, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, cond = None):\n",
    "        batch_size = x.shape[0]\n",
    "        out = (x.float() / (self.n_colors - 1) - 0.5) / 0.5\n",
    "        if self.conditional_size:\n",
    "            cond = self.cond_op(cond)\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, MaskConv2D):\n",
    "                out = layer(out, cond)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "            \n",
    "        if self.color_conditioning:\n",
    "            return out.view(batch_size, self.input_shape[0], self.color_channels,\n",
    "                          *self.input_shape[1:]).permute(0, 2, 1, 3, 4)\n",
    "        else:\n",
    "            return out.view(batch_size, self.color_channels, *self.input_shape)\n",
    "    \n",
    "    def loss(self, x, cond = None):\n",
    "        logits = self(x, cond)\n",
    "        loss = F.cross_entropy(logits, x.long())\n",
    "        return loss\n",
    " \n",
    "    def get_samples(self, n):\n",
    "        samples = torch.zeros([n, *self.input_shape]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.input_shape[1]):\n",
    "                for c in range(self.input_shape[2]):\n",
    "                    for k in range(self.input_shape[0]):\n",
    "                        out = self(samples)[:, :, k, r, c]\n",
    "                        probs = F.softmax(out, dim = 1)\n",
    "                        samples[:, k, r, c] = torch.multinomial(probs, 1).squeeze(-1)\n",
    "        return samples.permute(0, 2, 3, 1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEQo1iSMz1W0"
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for idx, (img, labels) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(img.to(device), cond = labels)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss\n",
    "\n",
    "def evaluate(model, testloader, optimizer, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, labels in enumerate(testloader):\n",
    "            loss = model.loss(img.to(device)).item()\n",
    "            test_loss += loss * img.shape[0]\n",
    "        test_loss /= len(testloader.dataset) # dividing by batch size\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "minibatch = 128\n",
    "d = 2\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root = './',\n",
    "                                                  train = True,\n",
    "                                                  download=True,\n",
    "                                                  transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test_dataset = torchvision.datasets.MNIST(root = './',\n",
    "                                                 train = False,\n",
    "                                                 download=True,\n",
    "                                                 transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "# preprocess dataset \n",
    "for idx, (image, label) in enumerate(train_dataset):\n",
    "    # binarize image\n",
    "    image[image < 0.5] = 0\n",
    "    image[image >= 0.5] = 1\n",
    "    # one hot encode labels\n",
    "    label_oh = np.zeros([1, n_classes])\n",
    "    label_oh[0][label] = 1\n",
    "    label_oh = torch.tensor(label_oh.astype('float32'))\n",
    "    \n",
    "for idx, (image, label) in enumerate(test_dataset):\n",
    "    # binarize image\n",
    "    image[image < 0.5] = 0\n",
    "    image[image >= 0.5] = 1\n",
    "    # one hot encode labels\n",
    "    label_oh = np.zeros([1, n_classes])\n",
    "    label_oh[0][label] = 1\n",
    "    label_oh = torch.tensor(label_oh.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "vDUBnZmcz6Pn",
    "outputId": "0319e7a5-ba2a-4dcf-bbbe-0b75db8d88af"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Relu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3b83fd42e555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                              \u001b[0muse_ResBlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                              \u001b[0mconditional_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                              device = device).to(device)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-993318e84873>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, channels, colors, no_of_layers, color_conditioning, use_ResBlock, conditional_size, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m             self.layers.extend([nn.ReLU(),\n\u001b[1;32m     32\u001b[0m                                 \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                 block(),])\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 2 1 x 1 Conv2D operation using Type B Mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-993318e84873>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Initialize block function to be used repeatedly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_ResBlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mResBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMaskConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconditional_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-17903c37a48d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         self.net = nn.ModuleList([\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mMaskConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_channels\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Relu'"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "minibatch = 128\n",
    "d = 2\n",
    "\n",
    "train_loss = np.zeros([minibatch * epochs, 1])\n",
    "test_loss = np.zeros([epochs, 1])\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(train_dataset, batch_size = minibatch, shuffle = True)\n",
    "testLoader = torch.utils.data.DataLoader(test_dataset, batch_size = minibatch, shuffle = True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Conditional_PixelCNN((1, 28, 28),\n",
    "                             channels = 64,\n",
    "                             colors = 2,\n",
    "                             no_of_layers = 3,\n",
    "                             color_conditioning=False,\n",
    "                             use_ResBlock = True,\n",
    "                             conditional_size = None,\n",
    "                             device = device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print (\"Epoch No. \" + str(epoch))\n",
    "    train_loss.extend(train(model, trainLoader, optimizer, device))\n",
    "    test_loss.append(evaluate(model, testLoader, optimizer, device)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-SwJfQzxjcD"
   },
   "outputs": [],
   "source": [
    "# s = model.get_samples(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "6IzuLHK5C9vy",
    "outputId": "cc95bef0-d96e-401c-83af-34758ea4cf93"
   },
   "outputs": [],
   "source": [
    "# size = 5\n",
    "# fig, axs = plt.subplots(size, size)\n",
    "# for i in range(0, size):\n",
    "#     for j in range(0, size):\n",
    "#         axs[i, j].imshow(s[size * i + j].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "H6Obhff40QCy",
    "outputId": "5e92f7e1-8c45-4cee-cac6-8368b2cd6fef"
   },
   "outputs": [],
   "source": [
    "# plt.plot(train_loss)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "kKBMtAvY4Ovs",
    "outputId": "bf314f99-9ba8-4d46-bed0-d1d5d4fc64fe"
   },
   "outputs": [],
   "source": [
    "# plt.plot(test_loss)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PixelCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2d7ad7bc45ed4927b29b80720d38cd19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3cf99f96d5af41a58530afcd23a2c696",
       "IPY_MODEL_886a6ed48fb249469557660d9f6c5e19"
      ],
      "layout": "IPY_MODEL_7323ce2032174e52b747c05642e32414"
     }
    },
    "3cf99f96d5af41a58530afcd23a2c696": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bcea843561c3450094b999b0f10d3ccb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_53ad61d9ae5e4320bf6db4b651ebc4d7",
      "value": 1
     }
    },
    "53ad61d9ae5e4320bf6db4b651ebc4d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7323ce2032174e52b747c05642e32414": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "886a6ed48fb249469557660d9f6c5e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa9d77592451452fb86ab1fee1d0830e",
      "placeholder": "​",
      "style": "IPY_MODEL_d482de3368704d4980cc1e8dcb32a318",
      "value": " 2640404480/? [03:36&lt;00:00, 8594772.41it/s]"
     }
    },
    "bcea843561c3450094b999b0f10d3ccb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d482de3368704d4980cc1e8dcb32a318": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa9d77592451452fb86ab1fee1d0830e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
